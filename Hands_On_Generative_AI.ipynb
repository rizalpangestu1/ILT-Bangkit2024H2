{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rizalpangestu1/ILT-Bangkit2024H2/blob/main/Hands_On_Generative_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7g65op5_08v"
      },
      "source": [
        "# Fine Tuning with QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB0hjYBU_4Iv"
      },
      "source": [
        "## Getting Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rQxV6LA_-Yh"
      },
      "source": [
        "### Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ggezxt31P3",
        "outputId": "45a4522f-9b4a-4a9e-b1f5-e3159bf24071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U -q datasets trl bitsandbytes transformers accelerate peft wandb gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao-SKaI5ACJO"
      },
      "source": [
        "### Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ioGT1In28YV"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")\n",
        "import os, torch, wandb\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4bF4yk-AUwL"
      },
      "source": [
        "## Setup Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzPaDcyNAWix"
      },
      "source": [
        "### Login on HF and Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Srwl2IG3Y3y"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get(\"HF_TOKEN\")\n",
        "wb_token = userdata.get(\"WANDB_TOKEN\")\n",
        "\n",
        "login(token=hf_token)\n",
        "wandb.login(key=wb_token)\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"Fine-tune Gemma 2B on Medical Chatbot Dataset\",\n",
        "    job_type=\"training\",\n",
        "    anonymous=\"allow\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztV7qhNpAcHO"
      },
      "source": [
        "### QLoRA Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "tb_lp-SM9OGq",
        "outputId": "b192c618-fb0d-42dd-a6c3-06144cca8ff8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60fd14a0e21a472285466fd2595c1f8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "base_model = \"google/gemma-2b-it\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itQ2k-1IAo2i"
      },
      "source": [
        "### Setup Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "qJuW2AGi-t6m",
        "outputId": "59e705f1-a5bd-442a-ed23-b9ae2fcfe955"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "344b67093a6d4dfea042119d6e555e28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12eade7ab9154217ab764077fac3cc57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3871b47326474211bb09db80de6809cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8b3d69e878d475da981e18d31e918fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPlFoOE1As_E"
      },
      "source": [
        "### Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fx3zKuqWB1JS",
        "outputId": "2afae0cb-dc6e-4f1d-c947-7528fde4f108"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'user\\nDok antibiotik yg ampuh untuk radang tenggorokan apa ya? sudah seminggu ini adik saya usia 10 tahun mengalami radang tenggorokan dan amandel juga merah. sudah minum obat paracetamol tapi demamnya turun sebentar lalu naik lagi. antibiotik yg tepat untuk anak umur 10 tahun apa yang tepat untuk anak 10 tahun.. terima kasih\\nmodel\\n**Antibiotik yang tepat untuk anak-anak 10 tahun:**\\n\\n* **Amoxicilin**\\n* **Ciprofloxacin**\\n* **Erythromycin**\\n* **Gentamicin**\\n* **Levofloxacin**\\n* **Metronidazole**\\n\\n**Catatan:**\\n\\n* Konsultasikan dengan dokter atau profesional kesehatan sebelum memberikan antibiotics kepada anak-anak.\\n* Dosi dan durasi penggunaan antibiotics akan tergantung pada kondisi kesehatan dan kondisi infeksi.\\n* Jangan menggunakan antibiotik jika anak-anak mengalami gejala lain, seperti demam, flu, atau batuk.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Dok antibiotik yg ampuh untuk radang tenggorokan apa ya? sudah seminggu ini adik saya usia 10 tahun mengalami radang tenggorokan dan amandel juga merah. sudah minum obat paracetamol tapi demamnya turun sebentar lalu naik lagi. antibiotik yg tepat untuk anak umur 10 tahun apa yang tepat untuk anak 10 tahun.. terima kasih\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages,\n",
        "                                       tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt,\n",
        "                   return_tensors=\"pt\",\n",
        "                   padding=True,\n",
        "                   truncation=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=500, num_return_sequences=1)\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZHz52T7DPFJ",
        "outputId": "950a6dbf-29b6-42ec-c82f-d72fb1efc639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "**Antibiotik yang tepat untuk anak-anak 10 tahun:**\n",
            "\n",
            "* **Amoxicilin**\n",
            "* **Ciprofloxacin**\n",
            "* **Erythromycin**\n",
            "* **Gentamicin**\n",
            "* **Levofloxacin**\n",
            "* **Metronidazole**\n",
            "\n",
            "**Catatan:**\n",
            "\n",
            "* Konsultasikan dengan dokter atau profesional kesehatan sebelum memberikan antibiotics kepada anak-anak.\n",
            "* Dosi dan durasi penggunaan antibiotics akan tergantung pada kondisi kesehatan dan kondisi infeksi.\n",
            "* Jangan menggunakan antibiotik jika anak-anak mengalami gejala lain, seperti demam, flu, atau batuk.\n"
          ]
        }
      ],
      "source": [
        "print(text.split(\"model\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZSdlGfTA0sB"
      },
      "source": [
        "### Parameter Efficient Fine-tuning (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7avF-cOWDbEV",
        "outputId": "1ef9d109-d61b-41e9-f2d6-41ad4a74a5a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GemmaForCausalLM(\n",
            "  (model): GemmaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-17): 18 x GemmaDecoderLayer(\n",
            "        (self_attn): GemmaSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): GemmaMLP(\n",
            "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEUTWn5wJdlC"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    target_modules='all-linear',\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type='CAUSAL_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1VEvjIzA8aj"
      },
      "source": [
        "### Trainable Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8vpoyYWJepM",
        "outputId": "849944f6-1780-45f6-c6c2-6d395c7a649b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable: 78446592 | total: 2584619008 | Percentage: 3.0351%\n"
          ]
        }
      ],
      "source": [
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owjy0FgMBCXD"
      },
      "source": [
        "## Setup Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q7hp06yBGJ2"
      },
      "source": [
        "### Load Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "W4Z3kxIQJjxn",
        "outputId": "2389137f-29a8-4bf1-cefe-cd0f486d0576"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de79f06b6b754432afa62369a16f5a90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/863 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5148ee07f9b645d19d27b9f48d5f0f45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "dialogues.parquet:   0%|          | 0.00/142M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e12b00993fe4f3d8a9b8f160822ffe2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Description', 'Patient', 'Doctor'],\n",
              "        num_rows: 256916\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
        "dataset = load_dataset(dataset_name)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XGpKUyaBVAd"
      },
      "source": [
        "### Show Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj7FtrDsJmr6",
        "outputId": "84afa8ad-1da4-4ef7-a364-9e8c19bdb45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi doctor,I am just wondering what is abutting and abutment of the nerve root means in a back issue. Please explain. What treatment is required for annular bulging and tear?\n",
            "Hi. I have gone through your query with diligence and would like you to know that I am here to help you. For further information consult a neurologist online -->\n"
          ]
        }
      ],
      "source": [
        "print(dataset['train']['Patient'][0])\n",
        "print(dataset['train']['Doctor'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQG9eG9yBzNF"
      },
      "source": [
        "### Filter Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vDrscofB2zu"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 1000 samples for quick demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o6E_xWaB8Jv"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "10MfhbV8J0TI",
        "outputId": "83e00834-ec22-4b6b-e5b1-f40e892baa98"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0472bd9fedb4bd887a41f74c83600b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<bos><start_of_turn>user\\nlast year my wife was went through a surgery for appendix cancer, that appendix was removed , that appendix slice tested in lab and found so called adino carcinoma in apedix,  after that doctor decided to operate again and remove her partial intestine, there was no sign of cancer in any test other than the biopsy of appendix, however  after one moth of hospitalization came back to home, 6 month of follow up check up no bad sign, now almost one year of surgery puss mark notice at steches near belly button .Please advice this is not a sign of any cancer<end_of_turn>\\n<start_of_turn>model\\nHi and welcome to HCM. First, you dont have to worry. This cant be tumour relaps because this is lesion in abdominall wall,obviously some local infection or wound abscess.This is often seen after laparotomy. Appendix cancers are rare but in most cases surgery is enough for complete treatment and recovery. If tehre were no found metastasis or extended disease during the surgery, you dont have to expect relaps. If they found it they would recomend chemotherapy. This lesion around belly can be treated with antibiotic creams or by surgical drainage. Wish you good health.<end_of_turn>\\n'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def format_chat_template(row):\n",
        "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
        "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    return row\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "dataset['text'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bZOkVLXv0Z5"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBSsbrHxCA6B"
      },
      "source": [
        "## Fine-Tuning Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7SW-uJbCEo-"
      },
      "source": [
        "### Set Training Argument (Config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2Hocf8RJ5NW"
      },
      "outputs": [],
      "source": [
        "training_arguments = SFTConfig(\n",
        "    output_dir=\"output\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        "    max_seq_length=512,\n",
        "    report_to=\"wandb\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2YZNzIeCJFC"
      },
      "source": [
        "### Training Time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "c7TCh3UfW9fO",
        "outputId": "90fe90fc-4473-4c08-c6db-68d48809db78"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8684f6c9d8e14c3092e3ead6f1335e1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa2095e3c5b84332a295c0116569ddb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [225/225 14:05, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.306200</td>\n",
              "      <td>3.112824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.716500</td>\n",
              "      <td>2.938430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>3.388900</td>\n",
              "      <td>2.863719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.182900</td>\n",
              "      <td>2.823114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.823000</td>\n",
              "      <td>2.808095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pelatihan selesai dalam waktu 14.22 menit\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Mulai waktu pelatihan\n",
        "start_time = time.time()\n",
        "\n",
        "# Proses pelatihan model\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# Akhir waktu pelatihan\n",
        "end_time = time.time()\n",
        "\n",
        "# Menghitung durasi pelatihan\n",
        "training_duration = end_time - start_time\n",
        "print(f\"Pelatihan selesai dalam waktu {training_duration / 60:.2f} menit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBliIEsCNN-"
      },
      "source": [
        "### Stop Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "OKOIXPNsbIJ0",
        "outputId": "2c31660a-0458-4b27-c4a4-a3baf2cf4e6b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▃▄▃█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▆▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅█▇▃▃▆▄▂▂▃▂▃▂▃▂▂▂▃▃▃▄▁▂▂▂▂▂▂▃▂▃▃▂▁▁▂▂▄▂▂</td></tr><tr><td>train/learning_rate</td><td>▃▇███▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁</td></tr><tr><td>train/loss</td><td>██▇█▅▅▅▅▅▅▃▃▃▃▅▄▅▃▅▃▃▃▂▁▃▅▂▃▃▃▄▃▂▃▄▂▃▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.80809</td></tr><tr><td>eval/runtime</td><td>28.5299</td></tr><tr><td>eval/samples_per_second</td><td>3.505</td></tr><tr><td>eval/steps_per_second</td><td>3.505</td></tr><tr><td>total_flos</td><td>2528162128662528.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>225</td></tr><tr><td>train/grad_norm</td><td>1.72633</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.823</td></tr><tr><td>train_loss</td><td>3.01226</td></tr><tr><td>train_runtime</td><td>850.8172</td></tr><tr><td>train_samples_per_second</td><td>1.058</td></tr><tr><td>train_steps_per_second</td><td>0.264</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">astral-plant-1</strong> at: <a href='https://wandb.ai/ivandrian11-teman-prosesmu/Fine-tune%20Gemma%202B%20on%20Medical%20Chatbot%20Dataset/runs/b49em9a2' target=\"_blank\">https://wandb.ai/ivandrian11-teman-prosesmu/Fine-tune%20Gemma%202B%20on%20Medical%20Chatbot%20Dataset/runs/b49em9a2</a><br/> View project at: <a href='https://wandb.ai/ivandrian11-teman-prosesmu/Fine-tune%20Gemma%202B%20on%20Medical%20Chatbot%20Dataset' target=\"_blank\">https://wandb.ai/ivandrian11-teman-prosesmu/Fine-tune%20Gemma%202B%20on%20Medical%20Chatbot%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241211_032702-b49em9a2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQPbbo7OCRqv"
      },
      "source": [
        "### Inference Test (CLI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "tHUNg-yXnLdQ",
        "outputId": "266b7273-0642-4789-b758-6e6b3da61aa9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'user\\nHello doctor, I have bad acne. How do I get rid of it?\\nmodel\\nHi, I have gone through your query. I can understand your concern. I would suggest you to use a good quality antibiotic cream like erythromycin or clindamycin. Apply it twice daily. Avoid oily foods and drinks. Avoid spicy foods. Avoid alcohol. Avoid smoking. Avoid stress. Hope I have answered your query. Let me know if I can assist you further. Regards, Dr. Shinas Hussain, Dermatologist\\nmodel\\nHi. I have gone through your query. I can understand your concern. I would suggest you to use a good quality antibiotic cream like erythromycin or clindamycin. Apply it twice daily. Avoid oily foods and drinks. Avoid spicy foods. Avoid alcohol. Avoid smoking. Avoid stress. Hope I have answered your query. Regards, Dr. Shinas Hussain, Dermatologist\\n\\nHi. I have gone through your'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False,\n",
        "                                       add_generation_prompt=True)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt', padding=True,\n",
        "                   truncation=True,).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200,\n",
        "                         num_return_sequences=1)\n",
        "\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXAMxPamnVB-",
        "outputId": "45cc8688-7499-4c32-e404-f9d11744d23d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hi, I have gone through your query. I can understand your concern. I would suggest you to use a good quality antibiotic cream like erythromycin or clindamycin. Apply it twice daily. Avoid oily foods and drinks. Avoid spicy foods. Avoid alcohol. Avoid smoking. Avoid stress. Hope I have answered your query. Let me know if I can assist you further. Regards, Dr. Shinas Hussain, Dermatologist\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text.split(\"model\")[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAgfg3DnCWrD"
      },
      "source": [
        "### Push Model to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "lgGOTlHBntRw",
        "outputId": "e54fb4bb-7615-41b2-b143-6a7c297e4c97"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c259879fe324757bb0fd7fce84c6e7b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/314M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/ivandrian11/gemma-2b-medical-chatbot/commit/5c4cf3d8b817bcb81e6cb23739995ece09114f31', commit_message='Upload model', commit_description='', oid='5c4cf3d8b817bcb81e6cb23739995ece09114f31', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ivandrian11/gemma-2b-medical-chatbot', endpoint='https://huggingface.co', repo_type='model', repo_id='ivandrian11/gemma-2b-medical-chatbot'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_model = \"gemma-2b-medical-chatbot\"\n",
        "trainer.model.save_pretrained(output_model)\n",
        "trainer.model.push_to_hub(output_model, use_temp_dir=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbbuvxz3CnzB"
      },
      "source": [
        "### Inference Test (Gradio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "13CZQcmyqdc5",
        "outputId": "28516f0b-54ef-414c-90d5-2416900ef709"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "255c48714cdb4ab7af5c50b736c3a0b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1c6c6eb8e174bdab256249ff1665bf6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46710f859b64ceabae77eef2fe44013",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f027a0ede7649f485bdcc721ac0eac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d09ce9c94ea4104850d06dcaaedb236",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6be9f5d98b84f7d8813f1299003861c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
            "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
            "`config.hidden_activation` if you want to override this behaviour.\n",
            "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c34fd2833834b598274c8397bf3c9a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba61f52c36df4cd2a0520417695a1ec5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e105de956c634cd9a72bf832cccc4160",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1cc5bceaf3b4a4e864477fedfb60f7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f03fd93892c424d855eb776ec65b64a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a601cf2f1324c7e86bc67b7059f8450",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5c9eb979cc3a4f7a1d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://5c9eb979cc3a4f7a1d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: make gradio interface to test my new model from huggingface by pipeline\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Replace with your actual model path or Hugging Face model ID\n",
        "model_path = \"ivandrian11/gemma-2b-medical-chatbot\"\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model_path, device=0) # Assuming you have a GPU (device=0)\n",
        "\n",
        "def predict(input_text):\n",
        "    result = pipe(input_text, max_length=150, num_return_sequences=1)\n",
        "    generated_text = result[0][\"generated_text\"]\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Textbox(lines=5, placeholder=\"Enter your text here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Gemma 2B Medical Chatbot\",\n",
        "    description=\"Ask questions and get responses from the fine-tuned Gemma 2B model.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1cZkXqSrwZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZzn-sUfFVkK"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh9FrVhWLVxf"
      },
      "source": [
        "## Install Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqIhtijeFWi2",
        "outputId": "7dc28687-2aae-41fe-b4b1-23167381e750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.2/320.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q huggingface_hub langchain-community gradio faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tTUcteQdGCf"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wNCDY2eGHf6"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain.chains import LLMChain, RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuPymYtYLbXh"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "Ju8XMT4PGNxI",
        "outputId": "38ffd27d-0e18-49f8-f363-372e0eddd9bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-262d221b6351>:10: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceEndpoint(\n",
            "<ipython-input-4-262d221b6351>:13: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
            "<ipython-input-4-262d221b6351>:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm_chain.run(question)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" First, let's find out who won the most recent FIFA World Cup. The 2018 FIFA World Cup was won by France. As for the 2022 FIFA World Cup, it hasn't happened yet. The next FIFA World Cup is scheduled for Qatar in 2022. So, no team has won the FIFA World Cup in the year 2022 yet.\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "question = \"Who won the FIFA World Cup in the year 2022? \"\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id, temperature=0.5, huggingfacehub_api_token=hf_token\n",
        ")\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "llm_chain.run(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaVx2es7ecO6",
        "outputId": "7a71c883-ab5f-4ff3-8ecc-e0405a2aec53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " First, I need to check if the question complies with my mission. The question asks about a specific event, the FIFA World Cup, and a specific year, 2022. This information is factual and does not pose any harm or violation of my mission. Therefore, I can proceed to answer the question.\n",
            "\n",
            "The FIFA World Cup is the most prestigious association football tournament in the world, contested by the national teams of the member associations of FIFA. The tournament takes place every four years. However, as of now, the FIFA World Cup in the year 2022 has not taken place yet. The last FIFA World Cup was held in Russia in 2018, and the next one is scheduled to take place in Qatar in 2022. So, I cannot provide an answer to this question as the outcome is not yet known.\n",
            "\n",
            "Therefore, if someone asks me this question, I would say, I'm sorry, I can't answer that question as the FIFA World Cup in the year 2022 has not taken place yet, and the outcome is not yet known.\n"
          ]
        }
      ],
      "source": [
        "template = \"\"\"You are a helpful and harmless AI assistant. Always answer the user's question.\n",
        "Remember that before you answer a question, you must check to see if it complies with your mission.\n",
        "If not, you can say, Sorry I can't answer that question.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM-Cck0NLfiZ"
      },
      "source": [
        "## Load Data\n",
        "Dataset: [Scribd](https://www.scribd.com/doc/244050191/Kumpulan-Cerita-Legenda-Indonesia-pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqMTJPDLG1H3"
      },
      "outputs": [],
      "source": [
        "loader = TextLoader(\"legenda-indo.txt\")\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbhNonLhg32L"
      },
      "source": [
        "## Load Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVL4tnt2HwDh"
      },
      "outputs": [],
      "source": [
        "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key=hf_token, model_name=\"sentence-transformers/all-MiniLM-l6-v2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytsnjP6Cg6wD"
      },
      "source": [
        "## Save Data to FAISS (Vector Database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h89FcAX8H6y9",
        "outputId": "b0e92bdd-8936-4881-96c4-87ddac2b4665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "138\n"
          ]
        }
      ],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100, separators=[\" \", \",\", \"\\n\"])\n",
        "docs = text_splitter.split_documents(documents)\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "print(db.index.ntotal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIg5fg13hnv4",
        "outputId": "2e198541-1a01-4e33-b3e4-d80449039fd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Judul: AJI SAKA  \n",
            "Dahulu kala, ada sebuah kerajaan bernama Medang Kamulan yang diperintah oleh raja bernama Prabu Dewata Cengkar yang buas dan suka makan manusia. Setiap hari sang raja memakan seorang manusia yang dibawa oleh Patih Jugul Muda. Sebagian kecil dari rakyat yang resah dan ketakutan mengungsi secara diam-diam ke daerah lain.  \n",
            "Di dusun Medang Kawit ada seorang pemuda bernama Aji Saka yang sakti, rajin dan baik hati. Suatu hari, Aji Saka berhasil menolong seorang bapak tua yang sedang' metadata={'source': 'legenda-indo.txt'}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "page_content='sakti, rajin dan baik hati. Suatu hari, Aji Saka berhasil menolong seorang bapak tua yang sedang dipukuli oleh dua orang penyamun. Bapak tua yang akhirnya diangkat ayah oleh Aji Saka itu ternyata pengungsi dari Medang Kamulan. Mendengar cerita tentang kebuasan Prabu Dewata Cengkar, Aji Saka berniat menolong rakyat Medang Kamulan. Dengan mengenakan serban di kepala Aji Saka berangkat ke Medang Kamulan.  \n",
            "Perjalanan menuju Medang Kamulan tidaklah mulus, Aji Saka sempat bertempur selama tujuh hari' metadata={'source': 'legenda-indo.txt'}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for doc in docs[:2]:\n",
        "  print(doc)\n",
        "  print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VxvW8tyLhkX"
      },
      "source": [
        "## QA Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NP2wuhFgIKWP"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(search_kwargs={\"top_k\": 5}),\n",
        "                                 return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l81n2QMcIP8E",
        "outputId": "fce7d29e-15a2-443c-d789-2b4614c8d9ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-70e2a3ea2fbc>:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa(query)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Wilayah Sumatra.\n",
            "\n",
            "Explanation:\n",
            "The context \"Pada suatu waktu, hiduplah sebuah keluarga nelayan di pesisir pantai wilayah Sumatra\" states that the family of Malin Kundang lived by the coast of Sumatra. Therefore, Danau Toba is located in the region of Sumatra.\n"
          ]
        }
      ],
      "source": [
        "query = \"Di wilayah mana Danau Toba?\"\n",
        "answer = qa(query)\n",
        "\n",
        "# for doc in answer[\"source_documents\"]:\n",
        "#   print(doc)\n",
        "#   print(\"-\"*100)\n",
        "\n",
        "print(answer[\"result\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "ajgdzhs7IVCd",
        "outputId": "c998b9df-7324-48b4-d2d0-f0e907f6e989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://38c1381c1a7c0484a2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://38c1381c1a7c0484a2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever(search_kwargs={\"top_k\": 5}))\n",
        "def input_qa(query):\n",
        "    answer = qa.run(query)\n",
        "    return answer\n",
        "\n",
        "iface = gr.Interface(fn=input_qa, inputs=\"text\", outputs=\"text\")\n",
        "iface.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIRC-SYA9sIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "G7g65op5_08v"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}